# ‚öôÔ∏è Configura√ß√£o: Cron para process-scraping-queue

## üéØ Objetivo

Executar `process-scraping-queue` **a cada 1 minuto** para processar a fila de scraping mais rapidamente.

---

## ‚úÖ Altera√ß√µes Implementadas

### **1. MAX_CONCURRENT aumentado para 30**

**Arquivo:** `supabase/functions/process-scraping-queue/index.ts`

**Antes:**
```typescript
const MAX_CONCURRENT = 10;
```

**Depois:**
```typescript
const MAX_CONCURRENT = 30;
```

**Impacto:**
- ‚úÖ Processa 30 leads simultaneamente (antes eram 10)
- ‚úÖ 3x mais r√°pido
- ‚úÖ Com fila de 2.149 mensagens: ~72 minutos (antes ~215 minutos)

---

## üîß Configura√ß√£o do Cron

### **Op√ß√£o 1: Supabase Dashboard (Recomendado)**

1. Acesse **Supabase Dashboard** > **Database** > **Cron Jobs**
2. Clique em **"New Cron Job"**
3. Configure:
   - **Name:** `process-scraping-queue-minute`
   - **Schedule:** `* * * * *` (a cada minuto)
   - **Command:** 
     ```sql
     SELECT net.http_post(
       url := 'https://nlbcwaxkeaddfocigwuk.supabase.co/functions/v1/process-scraping-queue',
       headers := jsonb_build_object(
         'Authorization', 'Bearer ' || current_setting('app.settings.service_role_key'),
         'Content-Type', 'application/json'
       ),
       body := jsonb_build_object('batch_size', 30)
     );
     ```

**‚ö†Ô∏è Nota:** Requer extens√£o `pg_net` habilitada.

---

### **Op√ß√£o 2: Cron Externo (GitHub Actions)**

**Arquivo:** `.github/workflows/process-scraping-queue.yml`

```yaml
name: Process Scraping Queue

on:
  schedule:
    - cron: '* * * * *'  # A cada minuto
  workflow_dispatch:  # Permite execu√ß√£o manual

jobs:
  process-scraping:
    runs-on: ubuntu-latest
    steps:
      - name: Call process-scraping-queue
        run: |
          curl -X POST https://${{ secrets.SUPABASE_URL }}/functions/v1/process-scraping-queue \
            -H "Authorization: Bearer ${{ secrets.SUPABASE_SERVICE_KEY }}" \
            -H "Content-Type: application/json" \
            -d '{"batch_size": 30}'
```

---

### **Op√ß√£o 3: Cron do Servidor (Linux/Mac)**

**Adicionar ao crontab (`crontab -e`):**

```bash
# Processar fila de scraping a cada minuto
* * * * * curl -X POST https://nlbcwaxkeaddfocigwuk.supabase.co/functions/v1/process-scraping-queue \
  -H "Authorization: Bearer [SERVICE_ROLE_KEY]" \
  -H "Content-Type: application/json" \
  -d '{"batch_size": 30}' > /dev/null 2>&1
```

---

### **Op√ß√£o 4: Usar pg_net (Se dispon√≠vel)**

**SQL para criar cron com pg_net:**

```sql
-- Habilitar pg_net
CREATE EXTENSION IF NOT EXISTS pg_net;

-- Criar fun√ß√£o que chama Edge Function
CREATE OR REPLACE FUNCTION call_process_scraping_queue()
RETURNS void
LANGUAGE plpgsql
SECURITY DEFINER
AS $$
DECLARE
  v_service_key text;
BEGIN
  -- Obter service key (precisa estar configurada)
  v_service_key := current_setting('app.settings.service_role_key', true);
  
  -- Chamar Edge Function via HTTP
  PERFORM net.http_post(
    url := 'https://nlbcwaxkeaddfocigwuk.supabase.co/functions/v1/process-scraping-queue',
    headers := jsonb_build_object(
      'Authorization', 'Bearer ' || v_service_key,
      'Content-Type', 'application/json'
    ),
    body := jsonb_build_object('batch_size', 30)
  );
END;
$$;

-- Criar cron job
SELECT cron.schedule(
  'process-scraping-queue-minute',
  '* * * * *',  -- A cada minuto
  'SELECT call_process_scraping_queue()'
);
```

---

## üìä Impacto Esperado

### **Antes:**
- **MAX_CONCURRENT:** 10
- **Frequ√™ncia:** ~5 minutos (assumindo)
- **Taxa:** ~2 leads/minuto
- **Tempo para processar 2.149 mensagens:** ~18 horas

### **Depois:**
- **MAX_CONCURRENT:** 30
- **Frequ√™ncia:** 1 minuto
- **Taxa:** ~30 leads/minuto
- **Tempo para processar 2.149 mensagens:** ~72 minutos (~1h12min)

**Melhoria:** ~15x mais r√°pido! üöÄ

---

## üîç Verificar Funcionamento

### **Ver logs da Edge Function:**
- Supabase Dashboard > Edge Functions > `process-scraping-queue` > Logs
- Verificar se est√° processando 30 leads por execu√ß√£o

### **Monitorar fila:**
```sql
SELECT 
  COUNT(*) as total_pendente,
  COUNT(*) FILTER (WHERE vt <= NOW()) as pronto_para_processar
FROM pgmq.q_scraping_queue;
```

### **Verificar taxa de processamento:**
```sql
SELECT 
  DATE_TRUNC('minute', updated_at) as minuto,
  COUNT(*) as leads_completados
FROM lead_extraction_staging
WHERE scraping_enriched = true
  AND updated_at >= NOW() - INTERVAL '1 hour'
GROUP BY DATE_TRUNC('minute', updated_at)
ORDER BY minuto DESC;
```

---

## ‚ö†Ô∏è Notas Importantes

1. **Scraping √© apenas de sites:** O `process-scraping-queue` processa apenas scraping de websites, n√£o inclui:
   - ‚ùå WHOIS (processado por `process-whois-queue`)
   - ‚ùå CNPJ (processado por `process-cnpj-queue`)
   - ‚úÖ Apenas scraping de sites

2. **Rate Limits:** Verificar se a API de scraping tem rate limits que podem ser atingidos com 30 requisi√ß√µes simult√¢neas.

3. **Custos:** Processar mais r√°pido pode aumentar custos da API de scraping.

4. **Monitoramento:** Acompanhar logs nas primeiras horas para garantir que est√° funcionando corretamente.

---

## ‚úÖ Pr√≥ximos Passos

1. ‚úÖ **Deploy da Edge Function** com MAX_CONCURRENT = 30
2. ‚è≥ **Configurar cron** para executar a cada minuto (escolher uma das op√ß√µes acima)
3. ‚è≥ **Monitorar** taxa de processamento nas pr√≥ximas horas
4. ‚è≥ **Ajustar** se necess√°rio (frequ√™ncia ou MAX_CONCURRENT)

---

**Status:** ‚úÖ MAX_CONCURRENT atualizado | ‚è≥ Cron precisa ser configurado

