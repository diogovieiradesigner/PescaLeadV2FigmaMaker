# üìä Monitoramento: Scraping com MAX_CONCURRENT = 60

## üéØ Configura√ß√£o Atual

**MAX_CONCURRENT:** `60` (aumentado de 30)  
**batch_size (cron):** `60`  
**Frequ√™ncia:** `1 minuto`  
**Taxa Esperada:** ~60 leads/minuto

---

## ‚ö†Ô∏è O que Monitorar

### **1. Taxa de Processamento**

**Verificar se est√° processando ~60 leads/minuto:**
```sql
SELECT 
  DATE_TRUNC('minute', updated_at) as minuto,
  COUNT(*) as leads_completados
FROM lead_extraction_staging
WHERE scraping_enriched = true
  AND updated_at >= NOW() - INTERVAL '1 hour'
GROUP BY DATE_TRUNC('minute', updated_at)
ORDER BY minuto DESC
LIMIT 10;
```

**Esperado:** ~60 leads por minuto

---

### **2. Tamanho da Fila**

**Verificar se a fila est√° diminuindo:**
```sql
SELECT 
  COUNT(*) as total_pendente,
  COUNT(*) FILTER (WHERE vt <= NOW()) as pronto_para_processar,
  COUNT(*) FILTER (WHERE vt > NOW()) as aguardando_timeout
FROM pgmq.q_scraping_queue;
```

**Esperado:** Fila diminuindo gradualmente

---

### **3. Taxa de Erros da API**

**Verificar logs da Edge Function:**
- Supabase Dashboard > Edge Functions > `process-scraping-queue` > Logs
- Procurar por:
  - `‚ùå [ERROR]` - Erros de API
  - `‚ö†Ô∏è [TIMEOUT]` - Timeouts
  - `Scraper API returned` - Erros HTTP

**Esperado:** Taxa de erro < 5%

---

### **4. Leads em Processamento Simult√¢neo**

**Verificar quantos leads est√£o sendo processados ao mesmo tempo:**
```sql
SELECT 
  COUNT(*) as sendo_processados_agora
FROM lead_extraction_staging
WHERE scraping_status = 'processing';
```

**Esperado:** At√© 60 simultaneamente

---

### **5. Tempo M√©dio de Processamento**

**Verificar se n√£o est√° demorando muito:**
```sql
SELECT 
  AVG(EXTRACT(EPOCH FROM (scraping_completed_at - scraping_started_at))) as tempo_medio_segundos,
  MAX(EXTRACT(EPOCH FROM (scraping_completed_at - scraping_started_at))) as tempo_maximo_segundos,
  COUNT(*) as total_completados
FROM lead_extraction_staging
WHERE scraping_enriched = true
  AND scraping_started_at IS NOT NULL
  AND scraping_completed_at IS NOT NULL
  AND scraping_completed_at >= NOW() - INTERVAL '1 hour';
```

**Esperado:** Tempo m√©dio < 30 segundos por lead

---

## üö® Sinais de Problema

### **1. Taxa de Erro Alta (> 10%)**
- **Sintoma:** Muitos erros nos logs
- **A√ß√£o:** Reduzir MAX_CONCURRENT para 40-50

### **2. Timeouts Frequentes**
- **Sintoma:** Muitos `TIMEOUT` nos logs
- **A√ß√£o:** Verificar se API est√° sobrecarregada, reduzir para 40-50

### **3. Fila N√£o Diminuindo**
- **Sintoma:** Fila continua crescendo mesmo com processamento
- **A√ß√£o:** Verificar se API est√° bloqueando requisi√ß√µes, reduzir para 30-40

### **4. Rate Limit da API**
- **Sintoma:** Erros 429 (Too Many Requests)
- **A√ß√£o:** Reduzir MAX_CONCURRENT para 30-40

---

## üìà M√©tricas Esperadas

### **Antes (MAX_CONCURRENT = 30):**
- Taxa: ~30 leads/minuto
- Tempo para 2.088 mensagens: ~70 minutos

### **Depois (MAX_CONCURRENT = 60):**
- Taxa: ~60 leads/minuto
- Tempo para 2.088 mensagens: ~35 minutos

**Melhoria:** 2x mais r√°pido! üöÄ

---

## üîß Ajustes R√°pidos (Se Necess√°rio)

### **Reduzir para 50:**
```typescript
const MAX_CONCURRENT = 50;
```

### **Reduzir para 40:**
```typescript
const MAX_CONCURRENT = 40;
```

### **Voltar para 30 (se houver problemas):**
```typescript
const MAX_CONCURRENT = 30;
```

**E atualizar cron:**
```sql
SELECT cron.unschedule('process-scraping-queue-v2');
SELECT cron.schedule(
  'process-scraping-queue-v2',
  '*/1 * * * *',
  $$
    SELECT net.http_post(
        url := 'https://nlbcwaxkeaddfocigwuk.supabase.co/functions/v1/process-scraping-queue',
        headers := jsonb_build_object(
            'Authorization', 'Bearer ' || (SELECT decrypted_secret FROM vault.decrypted_secrets WHERE name = 'service_role_key' LIMIT 1),
            'Content-Type', 'application/json'
        ),
        body := '{"batch_size": 30}'::jsonb  -- Ajustar aqui tamb√©m
    );
  $$
);
```

---

## ‚úÖ Checklist de Monitoramento

- [ ] Verificar taxa de processamento (deve ser ~60/min)
- [ ] Verificar tamanho da fila (deve estar diminuindo)
- [ ] Verificar logs de erro (deve ser < 5%)
- [ ] Verificar timeouts (deve ser m√≠nimo)
- [ ] Verificar se API est√° respondendo bem
- [ ] Acompanhar por 1-2 horas antes de considerar est√°vel

---

## üìù Notas

- **API de Scraping:** `https://proxy-scraper-api.diogo-vieira-pb-f91.workers.dev`
- **Timeout:** 180 segundos (3 minutos) por requisi√ß√£o
- **Frequ√™ncia:** Executa a cada 1 minuto

---

**Status:** ‚úÖ **Configurado para 60 leads simult√¢neos**

**Pr√≥ximo passo:** Deploy da Edge Function e monitoramento por 1-2 horas

